{"paragraphs":[{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1683230260354_-1618980245","id":"20230504-195740_940040772","dateCreated":"2023-05-04T19:57:40+0000","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:189","text":"%pyspark\r\nfrom pyspark.sql import SparkSession\r\n#SparkSession — way of initialization for basic PySpark functionality PySpark for creation of PySpark RDD, DataFrame and Dataset.\r\n#SparkSession internally creates SparkConfig and SparkContext\r\nspark = (SparkSession.builder\r\n.master(\"local[*]\")\r\n.appName('PySpark_Tutorial')\r\n.getOrCreate()\r\n)\r\n\r\ndept = [(\"Finance\",10), \r\n        (\"Marketing\",20), \r\n        (\"Sales\",30), \r\n        (\"IT\",40) \r\n      ]\r\ndeptColumns = [\"dept_name\",\"dept_id\"]\r\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\r\ndeptDF.printSchema()\r\ndeptDF.show(truncate=False)\r\ndeptDF.write.csv('dataset.csv')","dateUpdated":"2023-05-04T20:04:46+0000","dateFinished":"2023-05-04T20:04:47+0000","dateStarted":"2023-05-04T20:04:46+0000","errorMessage":""},{"text":"%pyspark\n\n","user":"anonymous","dateUpdated":"2023-05-04T20:02:24+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1683230313205_-547109560","id":"20230504-195833_1531594443","dateCreated":"2023-05-04T19:58:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3085","dateFinished":"2023-05-04T20:02:02+0000","dateStarted":"2023-05-04T20:02:01+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.17.0.5:4040/jobs/job?id=1","http://172.17.0.5:4040/jobs/job?id=2"],"interpreterSettingId":"spark"}}},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2023-05-04T20:00:49+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1683230449895_999754796","id":"20230504-200049_1039857222","dateCreated":"2023-05-04T20:00:49+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3191"}],"name":"test","id":"2HZN4V8PV","noteParams":{},"noteForms":{},"angularObjects":{},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}