DONT CONFUSE: Local PC repo  vs. VM(virt machine) repo vs. Hadoop file system
Hadoop is based on cluster. Cluster is collection of servers/=machines/=VMs. 
F.e.    1st VM - MasterNode
	2d  VM - DataNode
	3d  VM - DataNode
	4th VM - DataNode etc.
_______________________________________
	MAKE VM CONNECTION
_______________________________________
1. make sure you have files on your local repo for example: C:\Users\User1\Desktop\Karpov_DE\taxi_task
2. Run cluster DataProc (Hadoop) that contains several VM (Nodes)
3. Choose Host=VM (masterNode or DataNode) - Go to Network interface - Copy Public IPv4
4. Run CMD(Terminal) from local repo (where files are in)
5. run/connect to VM $ ssh root@84.201.186.95 (root - user\repo, numbers = Public IPv4)
6. see folders in /root/: $ ls
6.1 if needed make folders: $ mkdir /tmp/mapreduce
7. Exit from VM: $ exit
_______________________________________
	TRANSFER FILES from PC to VM
_______________________________________
1. after making connection to transfer files from local repo to VM:
$ scp PythonFile1.py root@84.201.186.118:/tmp/mapreduce/ (or scp ./*.py root@84.201.186.118:/tmp/mapreduce/ )
2. From VM to hadoop:  connect via ssh to VM
3. change current repo: $ cd <name_folder> (go to ex folder - just $ cd)
3. $ hadoop fs -put *.py input-data (its better to type all letters by hands, not copy-past)


Next code is about how to run 'RUN.SH' file to perform mapreduce:
$ export MR_OUTPUT=/user/root/output-data
$ root@rc1c-dataproc-m-edzzo1in56jbe2vq:~/tmp/mapreducer# hadoop fs -mkdir /user/root/output-data
$ root@rc1c-dataproc-m-edzzo1in56jbe2vq:~/tmp/mapreducer# hadoop fs -rm -r $MR_OUTPUT 
RAM (!! it should be directly written - delete = -rm)
$ root@rc1c-dataproc-m-edzzo1in56jbe2vq:~/tmp/mapreducer# hadoop jar "$HADOOP_MAPRED_HOME"/hadoop-streaming.jar -Dmapred.j
ob.name='Simple streaming job reduce' -file /tmp/mapreduce/mapper.py -mapper /tmp/mapreduce/mapper.py -file /tmp/mapredu
ce/reducer.py -reducer /tmp/mapreduce/reducer.py -input /user/root/input-data -output $MR_OUTPUT 
___________________________________________
	Tips:
___________________________________________
better to type all letters by hands, not copy-past
When its Error like No such file or directory - sometimes its just about changing slash sign to revers slash - / \ -even if prev path was written differentaly

___________________________________________
	TRANSFER FILES FROM S3 to VM, HDFS
___________________________________________
Для работы с amazon s3 установим утилиту awscli:
$ apt install -y awscli
Создадим именованный профиль AWS
aws configure --profile=a-sokolov-6
karpov-user - это имя профиля (можно выбрать любое)
Далее нас попросят ввести ключи, имя региона и формат вывода:
- AWS_ACCESS_KEY -- ***
- AWS_SECRET_KEY -- ***
- REGION -- ru-central1
- OUTPUT_FORMAT -- json
И теперь, чтобы посмотреть содержимое бакета, нужно выполнить команду:
$ aws --profile=a-sokolov-6 --endpoint-url=https://storage.yandexcloud.net s3 ls s3://ny-taxi-data/ny-taxi/
Download file:
$ aws --profile=a-sokolov-6 --endpoint-url=https://storage.yandexcloud.net s3 cp s3://ny-taxi-data/ny-taxi/yellow_tripdata_2020-12.csv ./
Загрузим скачанный файл на HDFS с размером блока 64Мб и двойной репликацией:
$ hadoop fs -Ddfs.blocksize=67108864 -Ddfs.replication=2 -put yellow_tripdata_2020-12.csv

$ aws --profile=a-sokolov-6 --endpoint-url=https://storage.yandexcloud.net s3 cp --recursive s3://ny-taxi-data/ny-taxi/ ./2020
И следующим шагом переложим их на HDFS тоже в директорию 2020:
$ hadoop fs -put /root/2020 2020
Чтобы посмотреть данные в конце файла воспользуемся tail:
hadoop fs -tail 2020/yellow_tripdata_2020-10.csv
_____________________________________________
FROM VM to S3 Object storage
____________________________________________
For created 'ais' service account
AWS Access Key ID: ID of a static key: YCAJEBG2YXEUNzhFeNtGFlIM0
AWS Secret Access Key: YCMTMgiq6bcCoQm735GeSqOefGEBZz3wWbDTRASC
Default region name: ru-central1
Format: Json

To upload=copy files from local repo to bucket:
$ aws --endpoint-url=https://storage.yandexcloud.net/ \
  s3 cp /home/ais/yellow_tripdata_2020-12.csv s3://bucketasokolov/122 (RAM: 112 -path_style_prefix future file_name in bucket)

optional:
we need to install utilite to use method "GET":
$ sudo apt install libwww-perl
See content in a bucket (json format):
$ GET https://bucketasokolov.storage.yandexcloud.net

$ aws s3 --endpoint-url=https://bucketasokolov.storage.yandexcloud.net ls

_____________________________________________
		from s3 to local repo (from venv)
_____________________________________________

RAM: Everithing below is in venv. Check:
> aws --version

cmd: Install or update the AWS CLI
> msiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi
check:
> aws --version
RAM: Installation 
> pip install awscli
check:
> aws --version
OUTPUT: aws-cli/1.27.71 Python/3.11.1 Windows/10 botocore/1.29.71

RAM: don't forget config:
aws configure --profile= karpov-user
- AWS_ACCESS_KEY -- ****
- AWS_SECRET_KEY -- ****
- REGION -- ru-central1
- OUTPUT_FORMAT -- json


RAM:  ./data - end local folder
> aws --profile=karpov-user --endpoint-url=https://storage.yandexcloud.net s3 cp s3://bucketasokolov/ye
llow_tripdata_2020-04.csv ./data


_____________________________________________
	MAP-REDUCE
_____________________________________________
PS C:\Users\Artemiy\Desktop\Karpov_DE\taxi_task> scp ./*.py root@84.201.186.95:
mapper.py                                                                             100%  266    13.6KB/s   00:00
reducer.py                                                                            100%  703    39.8KB/s   00:00
PS C:\Users\Artemiy\Desktop\Karpov_DE\taxi_task> scp ./*.sh root@84.201.186.95:
run.sh                                                                                100%  376    16.9KB/s   00:00
PS C:\Users\Artemiy\Desktop\Karpov_DE\taxi_task> ssh root@84.201.186.95
Welcome to Ubuntu 16.04.7 LTS (GNU/Linux 4.4.0-142-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage
root@rc1c-dataproc-m-edzzo1in56jbe2vq:~# hadoop fs -ls /user/root/
Found 6 items
drwxr-xr-x   - root hadoop          0 2023-02-02 16:18 /user/root/2020
-rw-r--r--   2 root hadoop         16 2023-02-02 16:00 /user/root/hello.txt
drwxr-xr-x   - root hadoop          0 2023-02-02 16:32 /user/root/input-data
-rw-r--r--   1 root hadoop          3 2023-02-02 18:12 /user/root/mapper.py
drwxr-xr-x   - root hadoop          0 2023-02-02 16:35 /user/root/tmp
-rw-r--r--   2 root hadoop  134481400 2023-02-02 16:14 /user/root/yellow_tripdata_2020-12.csv
root@rc1c-dataproc-m-edzzo1in56jbe2vq:~# hadoop fs -ls /user/root/input-data
Found 2 items
-rw-r--r--   1 root hadoop         16 2023-02-02 16:31 /user/root/input-data/hello.txt
-rw-r--r--   1 root hadoop  134481400 2023-02-02 16:32 /user/root/input-data/yellow_tripdata_2020-12.csv
root@rc1c-dataproc-m-edzzo1in56jbe2vq:~# hadoop fs –put *.py input-data/
–put: Unknown command

root@rc1c-dataproc-m-edzzo1in56jbe2vq:~# hadoop fs -put *.py /root/input-data
put: `/root/input-data': No such file or directory
root@rc1c-dataproc-m-edzzo1in56jbe2vq:~# hadoop fs -put *.py /input-data
put: `/input-data': No such file or directory
root@rc1c-dataproc-m-edzzo1in56jbe2vq:~# hadoop fs -put *.py /user/root/input-data
root@rc1c-dataproc-m-edzzo1in56jbe2vq:~# hadoop fs -ls /user/root/input-data
Found 4 items
-rw-r--r--   1 root hadoop         16 2023-02-02 16:31 /user/root/input-data/hello.txt
-rw-r--r--   1 root hadoop        266 2023-02-02 20:08 /user/root/input-data/mapper.py
-rw-r--r--   1 root hadoop        703 2023-02-02 20:08 /user/root/input-data/reducer.py
-rw-r--r--   1 root hadoop  134481400 2023-02-02 16:32 /user/root/input-data/yellow_tripdata_2020-12.csv
root@rc1c-dataproc-m-edzzo1in56jbe2vq:~# hadoop fs -put *.sh /user/root/input-data
root@rc1c-dataproc-m-edzzo1in56jbe2vq:~# hadoop fs -ls /user/root/input-data
Found 5 items
-rw-r--r--   1 root hadoop         16 2023-02-02 16:31 /user/root/input-data/hello.txt
-rw-r--r--   1 root hadoop        266 2023-02-02 20:08 /user/root/input-data/mapper.py
-rw-r--r--   1 root hadoop        703 2023-02-02 20:08 /user/root/input-data/reducer.py
-rw-r--r--   1 root hadoop        376 2023-02-02 20:09 /user/root/input-data/run.sh
-rw-r--r--   1 root hadoop  134481400 2023-02-02 16:32 /user/root/input-data/yellow_tripdata_2020-12.csv
root@rc1c-dataproc-m-edzzo1in56jbe2vq:~#